{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cefb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf192be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting code to run on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78891c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I told you.</td>\n",
       "      <td>Client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Told me what?</td>\n",
       "      <td>Therapist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That you'd be sorry you ever encouraged me to ...</td>\n",
       "      <td>Client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm not sorry at all.</td>\n",
       "      <td>Therapist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You didn't expect it to be like this, I bet.</td>\n",
       "      <td>Client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Like what?</td>\n",
       "      <td>Therapist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You know what? It's disappointing. I thought I...</td>\n",
       "      <td>Client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>And it isn't?</td>\n",
       "      <td>Therapist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>No, it's horrible. I don't know if I'm able to...</td>\n",
       "      <td>Client</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Are you all right?</td>\n",
       "      <td>Therapist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           utterance    speaker\n",
       "0                                        I told you.     Client\n",
       "1                                      Told me what?  Therapist\n",
       "2  That you'd be sorry you ever encouraged me to ...     Client\n",
       "3                              I'm not sorry at all.  Therapist\n",
       "4       You didn't expect it to be like this, I bet.     Client\n",
       "5                                         Like what?  Therapist\n",
       "6  You know what? It's disappointing. I thought I...     Client\n",
       "7                                      And it isn't?  Therapist\n",
       "8  No, it's horrible. I don't know if I'm able to...     Client\n",
       "9                                 Are you all right?  Therapist"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing data\n",
    "segmentation_df = pd.read_parquet(r\"C:\\Users\\sanke\\Desktop\\Therapist_Model\\Segmentation Data\\Data\\Final Data\\Therapy_Session.parquet\")\n",
    "segmentation_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5ef015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data: (21134, 2)\n",
      "The shape of the validation data: (4529, 2)\n",
      "The shape of the test data: (4529, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creating training, testing and validation data\n",
    "train_df, temp_df = train_test_split(segmentation_df, test_size=0.3, stratify=segmentation_df['speaker'], random_state=310)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['speaker'], random_state=310)\n",
    "print(f\"The shape of the training data: {train_df.shape}\")\n",
    "print(f\"The shape of the validation data: {val_df.shape}\")\n",
    "print(f\"The shape of the test data: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca6aed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data already exists at C:\\Users\\sanke\\Desktop\\Therapist_Model\\Segmentation Data\\Data\\Final Data\\Therapy_Session_Test.parquet, skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Save test data\n",
    "test_data_dir = r\"C:\\Users\\sanke\\Desktop\\Therapist_Model\\Segmentation Data\\Data\\Final Data\"\n",
    "os.makedirs(test_data_dir, exist_ok=True)\n",
    "test_data_path = os.path.join(test_data_dir, \"Therapy_Session_Test.parquet\")\n",
    "if not os.path.exists(test_data_path):\n",
    "    test_df.to_parquet(test_data_path, index=False)\n",
    "    print(f\"Test data saved to {test_data_path}\")\n",
    "else:\n",
    "    print(f\"Test data already exists at {test_data_path}, skipping save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56e68b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"label\"] = label_encoder.fit_transform(train_df[\"speaker\"])\n",
    "val_df[\"label\"] = label_encoder.transform(val_df[\"speaker\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2185f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "def tokenize_data(examples):\n",
    "    return tokenizer(examples[\"utterance\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1247f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrames to Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df[['utterance', 'label']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['utterance', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d157dc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca94bfd3d2f43c2a6cce45db58ea9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21134 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f9ab4389bf4b778521d0cec8810cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(tokenize_data, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eb79e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct format\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2eb6bed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\sanke\\anaconda3\\envs\\NLP_Project\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=7,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f818d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Callback for tracking metrics\n",
    "class MetricTrackerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            log_data = {\"epoch\": state.epoch}\n",
    "            log_data.update(logs)\n",
    "            self.logs.append(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81577757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3963' max='9247' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3963/9247 1:19:40 < 1:46:17, 0.83 it/s, Epoch 3/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.262700</td>\n",
       "      <td>0.216985</td>\n",
       "      <td>0.911239</td>\n",
       "      <td>0.911205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.261163</td>\n",
       "      <td>0.914330</td>\n",
       "      <td>0.914280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.181500</td>\n",
       "      <td>0.354650</td>\n",
       "      <td>0.905056</td>\n",
       "      <td>0.904885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3963, training_loss=0.21849208295631073, metrics={'train_runtime': 4781.9176, 'train_samples_per_second': 30.937, 'train_steps_per_second': 1.934, 'total_flos': 1.668176713193472e+16, 'train_loss': 0.21849208295631073, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate callback\n",
    "metric_tracker = MetricTrackerCallback()\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2), metric_tracker]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c1dedd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer and label encoder saved to C:\\Users\\sanke\\Desktop\\Therapist_Model\\Saved Model\n"
     ]
    }
   ],
   "source": [
    "# Save model, tokenizer and label encoder\n",
    "saved_model_dir = r\"C:\\Users\\sanke\\Desktop\\Therapist_Model\\Saved Model\"\n",
    "os.makedirs(saved_model_dir, exist_ok=True)\n",
    "model.save_pretrained(saved_model_dir)\n",
    "tokenizer.save_pretrained(saved_model_dir)\n",
    "joblib.dump(label_encoder, os.path.join(saved_model_dir, \"label_encoder.pkl\"))\n",
    "print(f\"Model, tokenizer and label encoder saved to {saved_model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
