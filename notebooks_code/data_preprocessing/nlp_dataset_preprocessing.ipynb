{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessing Datasets for NLP Emotion Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import torch\n",
    "from sklearn.utils import resample\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.augmenter.char as nac\n",
    "from deep_translator import GoogleTranslator\n",
    "from textattack.augmentation import EasyDataAugmenter\n",
    "from nltk.corpus import wordnet\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed, compute\n",
    "from tqdm import tqdm  # For progress tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ISEAR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion mapping to target 8 emotions\n",
    "TARGET_EMOTIONS = {\"Neutral\", \"Happy\", \"Anger\", \"Sadness\", \"Fear\", \"Surprise\", \"Confusion\", \"Disgust\"}\n",
    "\n",
    "iseardf = pd.read_csv(r'D:\\Data Science Projects\\AI Emotion Analysis\\data\\eng_dataset.csv')\n",
    "\n",
    "isear_emotion_mapping = {\n",
    "    \"anger\": \"Anger\",\n",
    "    \"disgust\": \"Disgust\",\n",
    "    \"fear\": \"Fear\",\n",
    "    \"happy\": \"Happy\",\n",
    "    \"sadness\": \"Sadness\",\n",
    "    \"surprise\": \"Surprise\",\n",
    "    \"joy\" : \"Happy\"\n",
    "}\n",
    "\n",
    "iseardf[\"emotion\"] = iseardf[\"sentiment\"].map(isear_emotion_mapping)\n",
    "iseardf = iseardf[iseardf[\"emotion\"].isin(TARGET_EMOTIONS)]\n",
    "iseardf = iseardf[[\"content\", \"emotion\"]]\n",
    "iseardf.rename(columns={\"content\": \"text\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. GoEmotions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"google-research-datasets/go_emotions\")\n",
    "\n",
    "goemotions_df = pd.concat([\n",
    "    pd.DataFrame(dataset[\"train\"]),\n",
    "    pd.DataFrame(dataset[\"validation\"]),\n",
    "    pd.DataFrame(dataset[\"test\"])\n",
    "], ignore_index=True)\n",
    "\n",
    "emotion_labels = dataset[\"train\"].features[\"labels\"].feature.names\n",
    "\n",
    "goemotions_emotion_mapping = {\n",
    "    \"neutral\": \"Neutral\",\n",
    "    \"admiration\": \"Happy\",\n",
    "    \"approval\": \"Happy\",\n",
    "    \"gratitude\": \"Happy\",\n",
    "    \"annoyance\": \"Anger\",\n",
    "    \"amusement\": \"Happy\",\n",
    "    \"curiosity\": \"Neutral\",\n",
    "    \"disapproval\": \"Anger\",\n",
    "    \"love\": \"Happy\",\n",
    "    \"optimism\": \"Happy\",\n",
    "    \"anger\": \"Anger\",\n",
    "    \"joy\": \"Happy\",\n",
    "    \"confusion\": \"Confusion\",\n",
    "    \"sadness\": \"Sadness\",\n",
    "    \"disappointment\": \"Sadness\",\n",
    "    \"realization\": \"Neutral\",\n",
    "    \"caring\": \"Happy\",\n",
    "    \"surprise\": \"Surprise\",\n",
    "    \"excitement\": \"Happy\",\n",
    "    \"disgust\": \"Disgust\",\n",
    "    \"desire\": \"Neutral\",\n",
    "    \"fear\": \"Fear\",\n",
    "    \"remorse\": \"Sadness\",\n",
    "    \"embarrassment\": \"Fear\",\n",
    "    \"nervousness\": \"Fear\",\n",
    "    \"relief\": \"Neutral\",\n",
    "    \"pride\": \"Happy\",\n",
    "    \"grief\": \"Sadness\"\n",
    "}\n",
    "\n",
    "goemotions_df[\"emotion\"] = goemotions_df[\"labels\"].apply(lambda labels: [goemotions_emotion_mapping[emotion_labels[i]] for i in labels])\n",
    "goemotions_df = goemotions_df.explode(\"emotion\")\n",
    "goemotions_df = goemotions_df[goemotions_df[\"emotion\"].isin(TARGET_EMOTIONS)]  \n",
    "goemotions_df = goemotions_df[[\"text\", \"emotion\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. DailyDialog Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_dialog = load_dataset(\"daily_dialog\", trust_remote_code=True)\n",
    "\n",
    "emotion_mapping = {\n",
    "    0: \"Neutral\",\n",
    "    1: \"Anger\",\n",
    "    2: \"Disgust\",\n",
    "    3: \"Fear\",\n",
    "    4: \"Happy\",\n",
    "    5: \"Sadness\",\n",
    "    6: \"Surprise\"\n",
    "}\n",
    "\n",
    "TARGET_EMOTIONS = [\"Neutral\", \"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Sadness\", \"Surprise\"]\n",
    "\n",
    "def preprocess_split(split):\n",
    "    data = []\n",
    "    for example in split:\n",
    "        dialog = example[\"dialog\"]  \n",
    "        emotions = example[\"emotion\"]  \n",
    "        \n",
    "        for utterance, emotion in zip(dialog, emotions):\n",
    "            utterance = utterance.strip()\n",
    "            if len(utterance.split()) < 10:\n",
    "                continue\n",
    "            emotion_name = emotion_mapping[emotion]\n",
    "            if TARGET_EMOTIONS and emotion_name not in TARGET_EMOTIONS:\n",
    "                continue\n",
    "            data.append({\n",
    "                \"text\": utterance,\n",
    "                \"emotion\": emotion_name\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_df = preprocess_split(daily_dialog[\"train\"])\n",
    "test_df = preprocess_split(daily_dialog[\"test\"])\n",
    "val_df = preprocess_split(daily_dialog[\"validation\"])\n",
    "\n",
    "combined_df_dailydialog = pd.concat([train_df, test_df, val_df], ignore_index=True)\n",
    "combined_df_dailydialog = combined_df_dailydialog[combined_df_dailydialog[\"emotion\"].isin(TARGET_EMOTIONS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ESConv Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliir\\AppData\\Local\\Temp\\ipykernel_22980\\735548738.py:49: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  conversation = json.loads(row[0])\n"
     ]
    }
   ],
   "source": [
    "TARGET_EMOTIONS = {\"Neutral\", \"Happy\", \"Anger\", \"Sadness\", \"Fear\", \"Surprise\", \"Confusion\", \"Disgust\"}\n",
    "\n",
    "EMOTION_LABELS = {\n",
    "    \"joy\": \"Happy\",\n",
    "    \"happy\": \"Happy\",\n",
    "    \"excited\": \"Happy\",\n",
    "    \n",
    "    \"anger\": \"Anger\",\n",
    "    \"angry\": \"Anger\",\n",
    "    \"frustrated\": \"Anger\",\n",
    "    \"jealousy\": \"Anger\",  \n",
    "    \n",
    "    \"sad\": \"Sadness\",\n",
    "    \"sadness\": \"Sadness\",\n",
    "    \"depression\": \"Sadness\",\n",
    "    \"guilt\": \"Sadness\",\n",
    "    \"pain\": \"Sadness\",\n",
    "\n",
    "    \"fear\": \"Fear\",\n",
    "    \"anxiety\": \"Fear\",\n",
    "    \"nervousness\": \"Fear\",\n",
    "\n",
    "    \"surprise\": \"Surprise\",\n",
    "    \n",
    "    \"disgust\": \"Disgust\",\n",
    "    \"shame\": \"Disgust\",\n",
    "\n",
    "    \"neutral\": \"Neutral\",\n",
    "\n",
    "    \"confused\": \"Confusion\",\n",
    "    \"confusion\": \"Confusion\"\n",
    "}\n",
    "\n",
    "def standardize_emotions(label):\n",
    "    \"\"\"Map dataset-specific emotion labels to a unified set, keeping only relevant categories.\"\"\"\n",
    "    mapped_label = EMOTION_LABELS.get(label.lower(), None)\n",
    "    return mapped_label if mapped_label in TARGET_EMOTIONS else None\n",
    "\n",
    "dataset = load_dataset(\"thu-coai/ESConv\")\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.DataFrame(dataset[\"train\"]),\n",
    "    pd.DataFrame(dataset[\"validation\"]),\n",
    "    pd.DataFrame(dataset[\"test\"])\n",
    "])\n",
    "\n",
    "seeker_texts = []\n",
    "for index, row in df.iterrows():\n",
    "    conversation = json.loads(row[0])  \n",
    "\n",
    "    if \"emotion_type\" in conversation and \"dialog\" in conversation:\n",
    "        emotion = conversation[\"emotion_type\"]  \n",
    "        dialog = conversation[\"dialog\"]  \n",
    "\n",
    "        \n",
    "        seeker_messages = \" \".join(turn[\"text\"] for turn in dialog if turn[\"speaker\"] == \"usr\")\n",
    "\n",
    "        \n",
    "        seeker_texts.append({\n",
    "            \"conversation_id\": index,  \n",
    "            \"text\": seeker_messages,   \n",
    "            \"emotion\": emotion        \n",
    "        })\n",
    "\n",
    "\n",
    "df_seeker = pd.DataFrame(seeker_texts)\n",
    "df_seeker[\"emotion\"] = df_seeker[\"emotion\"].map(standardize_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Dair-ai Emotion Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dair-ai/emotion\",\"unsplit\")\n",
    "\n",
    "# mapping\n",
    "original_labels = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\"\n",
    "}\n",
    "\n",
    "# Mapping to new emotions\n",
    "new_label_mapping = {\n",
    "    \"joy\": \"Happy\",\n",
    "    \"sadness\": \"Sadness\",\n",
    "    \"anger\": \"Anger\",\n",
    "    \"fear\": \"Fear\",\n",
    "    \"surprise\": \"Surprise\",\n",
    "    \"love\": \"Neutral\" \n",
    "}\n",
    "\n",
    "def map_labels(example):\n",
    "    example[\"new_label\"] = new_label_mapping[original_labels[example[\"label\"]]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(map_labels)\n",
    "data_emotion = pd.DataFrame(dataset['train'])\n",
    "data_emotion = data_emotion[['text','new_label']]\n",
    "data_emotion.columns = ['text','emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i feel awful about it too because it s my job ...</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im alone i feel awful</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ive probably mentioned this before but i reall...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was feeling a little low few days back</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i beleive that i am much more sensitive to oth...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416804</th>\n",
       "      <td>that was what i felt when i was finally accept...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416805</th>\n",
       "      <td>i take every day as it comes i m just focussin...</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416806</th>\n",
       "      <td>i just suddenly feel that everything was fake</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416807</th>\n",
       "      <td>im feeling more eager than ever to claw back w...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416808</th>\n",
       "      <td>i give you plenty of attention even when i fee...</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416809 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  emotion\n",
       "0       i feel awful about it too because it s my job ...  Sadness\n",
       "1                                   im alone i feel awful  Sadness\n",
       "2       ive probably mentioned this before but i reall...    Happy\n",
       "3                i was feeling a little low few days back  Sadness\n",
       "4       i beleive that i am much more sensitive to oth...  Neutral\n",
       "...                                                   ...      ...\n",
       "416804  that was what i felt when i was finally accept...    Happy\n",
       "416805  i take every day as it comes i m just focussin...     Fear\n",
       "416806      i just suddenly feel that everything was fake  Sadness\n",
       "416807  im feeling more eager than ever to claw back w...    Happy\n",
       "416808  i give you plenty of attention even when i fee...  Sadness\n",
       "\n",
       "[416809 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text emotion\n",
      "0  At the point today where if someone says somet...   Anger\n",
      "1  @CorningFootball  IT'S GAME DAY!!!!      T MIN...   Anger\n",
      "2  This game has pissed me off more than any othe...   Anger\n",
      "3  @spamvicious I've just found out it's Candice ...   Anger\n",
      "4  @moocowward @mrsajhargreaves @Melly77 @GaryBar...   Anger\n",
      "     Emotion   Count\n",
      "2      Happy  163167\n",
      "3    Sadness  124141\n",
      "4    Neutral   93354\n",
      "0      Anger   63782\n",
      "1       Fear   47515\n",
      "5   Surprise   14211\n",
      "6  Confusion    1535\n",
      "7    Disgust    1010\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.concat([iseardf, goemotions_df, combined_df_dailydialog,df_seeker[['text','emotion']],data_emotion], ignore_index=True)\n",
    "final_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "print(final_df.head())\n",
    "\n",
    "emotion_counts = Counter(final_df[\"emotion\"])\n",
    "emotion_df = pd.DataFrame(emotion_counts.items(), columns=[\"Emotion\", \"Count\"]).sort_values(by=\"Count\", ascending=False)\n",
    "print(emotion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>At the point today where if someone says somet...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@CorningFootball  IT'S GAME DAY!!!!      T MIN...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This game has pissed me off more than any othe...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@spamvicious I've just found out it's Candice ...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@moocowward @mrsajhargreaves @Melly77 @GaryBar...</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508710</th>\n",
       "      <td>that was what i felt when i was finally accept...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508711</th>\n",
       "      <td>i take every day as it comes i m just focussin...</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508712</th>\n",
       "      <td>i just suddenly feel that everything was fake</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508713</th>\n",
       "      <td>im feeling more eager than ever to claw back w...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508714</th>\n",
       "      <td>i give you plenty of attention even when i fee...</td>\n",
       "      <td>Sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>508715 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  emotion\n",
       "0       At the point today where if someone says somet...    Anger\n",
       "1       @CorningFootball  IT'S GAME DAY!!!!      T MIN...    Anger\n",
       "2       This game has pissed me off more than any othe...    Anger\n",
       "3       @spamvicious I've just found out it's Candice ...    Anger\n",
       "4       @moocowward @mrsajhargreaves @Melly77 @GaryBar...    Anger\n",
       "...                                                   ...      ...\n",
       "508710  that was what i felt when i was finally accept...    Happy\n",
       "508711  i take every day as it comes i m just focussin...     Fear\n",
       "508712      i just suddenly feel that everything was fake  Sadness\n",
       "508713  im feeling more eager than ever to claw back w...    Happy\n",
       "508714  i give you plenty of attention even when i fee...  Sadness\n",
       "\n",
       "[508715 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(r'D:\\Data Science Projects\\AI Emotion Analysis\\data\\merged_dataset_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Validation Split Before UpSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "final_df = pd.read_csv(r'D:\\Data Science Projects\\AI Emotion Analysis\\data\\merged_dataset_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Distribution: Counter({'Happy': 132165, 'Sadness': 100554, 'Neutral': 75617, 'Anger': 51663, 'Fear': 38487, 'Surprise': 11511, 'Confusion': 1243, 'Disgust': 818})\n",
      "Validation Distribution: Counter({'Happy': 14685, 'Sadness': 11173, 'Neutral': 8402, 'Anger': 5741, 'Fear': 4276, 'Surprise': 1279, 'Confusion': 138, 'Disgust': 91})\n",
      "Test Distribution: Counter({'Happy': 16317, 'Sadness': 12414, 'Neutral': 9335, 'Anger': 6378, 'Fear': 4752, 'Surprise': 1421, 'Confusion': 154, 'Disgust': 101})\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    final_df, \n",
    "    test_size=0.1,  \n",
    "    random_state=42, \n",
    "    stratify=final_df[\"emotion\"]  \n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_df, \n",
    "    test_size=0.1,      \n",
    "    random_state=42, \n",
    "    stratify=train_df[\"emotion\"]  \n",
    ")\n",
    "\n",
    "print(\"Train Distribution:\", Counter(train_df[\"emotion\"]))\n",
    "print(\"Validation Distribution:\", Counter(val_df[\"emotion\"]))\n",
    "print(\"Test Distribution:\", Counter(test_df[\"emotion\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(r'D:\\Data Science Projects\\AI Emotion Analysis\\data\\train.csv', index=False)\n",
    "val_df.to_csv(r'D:\\Data Science Projects\\AI Emotion Analysis\\data\\val.csv', index=False)\n",
    "test_df.to_csv(r'D:\\Data Science Projects\\AI Emotion Analysis\\data\\test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance - Augmentation Strategy for Balanced Emotion Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aliir\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\aliir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Device set to use cuda:0\n",
      "Processing Emotion: Anger:  50%|█████     | 4/8 [00:00<00:00, 37.45it/s]  You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Processing Emotion: Anger:  50%|█████     | 4/8 [00:15<00:00, 37.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Back-translation failed: None --> text must be a valid text with maximum 5000 character,otherwise it cannot be translated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting Texts: 100%|██████████| 8645/8645 [53:28<00:00,  2.69it/s]\n",
      "Processing Emotion: Confusion:  88%|████████▊ | 7/8 [53:28<09:13, 553.14s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Augmenting Texts: 100%|██████████| 8933/8933 [2:10:04<00:00,  1.14it/s]\n",
      "Processing Emotion: Disgust: 100%|██████████| 8/8 [3:03:32<00:00, 1376.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after upsampling: Counter({'Happy': 140742, 'Sadness': 103781, 'Neutral': 86949, 'Anger': 54677, 'Fear': 42027, 'Surprise': 13989, 'Confusion': 10000, 'Disgust': 10000})\n"
     ]
    }
   ],
   "source": [
    "# Checking for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Augmentation\n",
    "syn_aug = naw.SynonymAug(aug_p=0.3)\n",
    "\n",
    "device_id = 0 if torch.cuda.is_available() else -1  # Ensure device is an integer\n",
    "bert_aug = naw.ContextualWordEmbsAug(model_path=\"bert-base-uncased\", action=\"substitute\", device=device_id)\n",
    "\n",
    "del_aug = naw.RandomWordAug(action=\"delete\", aug_p=0.2)\n",
    "\n",
    "back_trans_aug = naw.BackTranslationAug(\n",
    "    from_model_name=\"Helsinki-NLP/opus-mt-en-fr\",\n",
    "    to_model_name=\"Helsinki-NLP/opus-mt-fr-en\"\n",
    ")\n",
    "\n",
    "eda_augmenter = EasyDataAugmenter()\n",
    "\n",
    "# Paraphrasing model using GPU\n",
    "paraphrase_model = pipeline(\"text2text-generation\", model=\"t5-small\", device=device_id)\n",
    "\n",
    "augmenters = [syn_aug, bert_aug, del_aug, back_trans_aug, eda_augmenter]\n",
    "\n",
    "# Custom augmentations\n",
    "def synonym_replacement(text, n=2):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_indices = [i for i, word in enumerate(words) if wordnet.synsets(word)]\n",
    "    random.shuffle(random_indices)\n",
    "    replaced = 0\n",
    "    for i in random_indices:\n",
    "        if replaced >= n:\n",
    "            break\n",
    "        synonyms = wordnet.synsets(words[i])\n",
    "        if synonyms:\n",
    "            lemmas = synonyms[0].lemmas()\n",
    "            if lemmas:\n",
    "                new_words[i] = lemmas[0].name()\n",
    "                replaced += 1\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def back_translate(text, target_lang='fr'):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return text\n",
    "    try:\n",
    "        translated = GoogleTranslator(source='en', target=target_lang).translate(text)\n",
    "        back_translated = GoogleTranslator(source=target_lang, target='en').translate(translated)\n",
    "        return back_translated if back_translated else text\n",
    "    except Exception as e:\n",
    "        print(f\"Back-translation failed: {e}\")\n",
    "        return text\n",
    "\n",
    "def paraphrase_text(text):\n",
    "    try:\n",
    "        inputs = f\"paraphrase: {text}\"\n",
    "        result = paraphrase_model(inputs, max_length=100, do_sample=True)\n",
    "        return result[0]['generated_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Paraphrasing failed: {e}\")\n",
    "        return text\n",
    "\n",
    "def apply_augmenter(text, augmenter):\n",
    "    augmented = augmenter.augment(text)\n",
    "    return augmented if isinstance(augmented, str) else text\n",
    "\n",
    "def augment_text(text):\n",
    "    methods = [\n",
    "        synonym_replacement,\n",
    "        back_translate,\n",
    "        paraphrase_text,\n",
    "        lambda x: apply_augmenter(x, random.choice(augmenters))\n",
    "    ]\n",
    "    random.shuffle(methods)\n",
    "    augmented_text = str(text)\n",
    "    for method in methods[:2]:\n",
    "        try:\n",
    "            augmented_text = method(augmented_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Augmentation failed: {e}\")\n",
    "    return augmented_text\n",
    "\n",
    "def augment_with_dask(texts, n_partitions=4):\n",
    "    \"\"\"\n",
    "    Augments a list of texts in parallel using Dask.\n",
    "    \"\"\"\n",
    "    ddf = dd.from_pandas(pd.DataFrame({\"text\": texts}), npartitions=n_partitions)\n",
    "    meta = (\"text\", \"object\")  # Output will be a Series with text data\n",
    "\n",
    "    ddf[\"augmented_text\"] = ddf[\"text\"].map(augment_text,meta = meta)\n",
    "    \n",
    "    # Compute the result with a progress bar\n",
    "    with tqdm(total=len(texts), desc=\"Augmenting Texts\") as pbar:\n",
    "        augmented_df = ddf.compute()\n",
    "        pbar.update(len(texts))  # Update progress bar\n",
    "    \n",
    "    return augmented_df[\"augmented_text\"].tolist()\n",
    "\n",
    "def balance_specific_emotions(train_df, target_size=10000, n_partitions=4):\n",
    "    \"\"\"\n",
    "    Upsamples only the 'Confusion' and 'Disgust' classes using Dask-based parallel augmentation.\n",
    "    Other classes are retained as-is.\n",
    "    \"\"\"\n",
    "    balanced_data = []\n",
    "    emotions_to_upsample = ['Confusion', 'Disgust']\n",
    "    all_emotions = train_df[\"emotion\"].unique()\n",
    "\n",
    "    pbar = tqdm(all_emotions, desc=\"Processing Emotions\")\n",
    "\n",
    "    for emotion in pbar:\n",
    "        group = train_df[train_df[\"emotion\"] == emotion]\n",
    "        \n",
    "        if emotion in emotions_to_upsample:\n",
    "            # Upsample if the emotion is 'Confusion' or 'Disgust'\n",
    "            if len(group) < target_size:\n",
    "                additional = target_size - len(group)\n",
    "                resampled = resample(group[\"text\"], n_samples=additional, random_state=42)\n",
    "                \n",
    "                augmented = augment_with_dask(resampled, n_partitions=n_partitions)\n",
    "                augmented_df = pd.DataFrame({\n",
    "                    \"text\": augmented,\n",
    "                    \"emotion\": [emotion] * additional\n",
    "                })\n",
    "                group = pd.concat([group, augmented_df])\n",
    "        balanced_data.append(group)\n",
    "\n",
    "        pbar.set_description(f\"Processing Emotion: {emotion}\")\n",
    "        pbar.refresh()\n",
    "\n",
    "    pbar.close()  \n",
    "    return pd.concat(balanced_data).reset_index(drop=True)\n",
    "\n",
    "# Upsampling 'Confusion' and 'Disgust' classes because of class imbalance.\n",
    "\n",
    "balanced_df = balance_specific_emotions(train_df, target_size=10000, n_partitions=4)\n",
    "print(\"Class distribution after upsampling:\", Counter(balanced_df[\"emotion\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.to_csv(r'D:\\Data Science Projects\\AI Emotion Analysis\\data\\train_upsampled.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
